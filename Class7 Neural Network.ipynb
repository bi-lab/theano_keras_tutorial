{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import h5py\n",
    "floatX = theano.config.floatX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data to VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5f = h5py.File('mnist.hdf5','r')\n",
    "data_x = theano.shared(h5f['train_data'][()])\n",
    "data_y = theano.shared(np.asarray(h5f['train_label'][()], dtype='int32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create parameters in VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1 = theano.shared(np.ones((784, 300),dtype=floatX),name='w1')\n",
    "w2 = theano.shared(np.ones((300, 10), dtype=floatX),name='w2')\n",
    "b1 = theano.shared(np.zeros((300,), dtype=floatX),name='b1')\n",
    "b2 = theano.shared(np.zeros((10,), dtype=floatX),name='b2')\n",
    "params = [w1,w2,b1,b2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "$h_1=\\sigma(w_1 x+b_1)$\n",
    "\n",
    "$y=\\sigma(w_2 h_1+b_2)$\n",
    "\n",
    "$cost = -\\frac{1}{N}\\sum\\limits_{i=1}^N \\log y_i^{t_i} $ (Cross Entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = T.matrix('x')\n",
    "t = T.ivector('t')\n",
    "h1 = T.nnet.sigmoid(T.dot(x,w1)+b1)\n",
    "y = T.nnet.softmax(T.dot(h1,w2)+b2)\n",
    "cost = -T.mean(T.log(y)[T.arange(t.shape[0]), t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = T.iscalar('i')\n",
    "batch_size = 200\n",
    "n_batches = int(data_x.get_value().shape[0] / batch_size)\n",
    "\n",
    "grads = T.grad(cost, params)\n",
    "nu = 0.01\n",
    "updates = [(p, p-nu*grad) for p,grad in zip(params, grads)]\n",
    "train_model = theano.function([i], cost, updates=updates, \n",
    "                              givens=[(x,data_x[i*batch_size:(i+1)*batch_size]),\n",
    "                                      (t,data_y[i*batch_size:(i+1)*batch_size])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Cost 2.30424356461\n",
      "Epoch 1 Cost 2.30423760414\n",
      "Epoch 2 Cost 2.30423736572\n",
      "Epoch 3 Cost 2.30423688889\n",
      "Epoch 4 Cost 2.30423760414\n",
      "Epoch 5 Cost 2.30423688889\n",
      "Epoch 6 Cost 2.30423855782\n",
      "Epoch 7 Cost 2.30423760414\n",
      "Epoch 8 Cost 2.30423593521\n",
      "Epoch 9 Cost 2.30423855782\n",
      "Epoch 10 Cost 2.30423736572\n",
      "Epoch 11 Cost 2.30423688889\n",
      "Epoch 12 Cost 2.30423927307\n",
      "Epoch 13 Cost 2.30423688889\n",
      "Epoch 14 Cost 2.30423736572\n",
      "Epoch 15 Cost 2.30423545837\n",
      "Epoch 16 Cost 2.30423736572\n",
      "Epoch 17 Cost 2.30423641205\n",
      "Epoch 18 Cost 2.30423784256\n",
      "Epoch 19 Cost 2.30423593521\n",
      "Epoch 20 Cost 2.30423736572\n",
      "Epoch 21 Cost 2.30423784256\n",
      "Epoch 22 Cost 2.30423927307\n",
      "Epoch 23 Cost 2.3042383194\n",
      "Epoch 24 Cost 2.30423736572\n",
      "Epoch 25 Cost 2.30423688889\n",
      "Epoch 26 Cost 2.30423855782\n",
      "Epoch 27 Cost 2.30423641205\n",
      "Epoch 28 Cost 2.30423784256\n",
      "Epoch 29 Cost 2.30423784256\n",
      "Epoch 30 Cost 2.30423808098\n",
      "Epoch 31 Cost 2.3042383194\n",
      "Epoch 32 Cost 2.3042371273\n",
      "Epoch 33 Cost 2.3042371273\n",
      "Epoch 34 Cost 2.30423736572\n",
      "Epoch 35 Cost 2.30423760414\n",
      "Epoch 36 Cost 2.30423760414\n",
      "Epoch 37 Cost 2.30423641205\n",
      "Epoch 38 Cost 2.30423927307\n",
      "Epoch 39 Cost 2.30423688889\n",
      "Epoch 40 Cost 2.3042371273\n",
      "Epoch 41 Cost 2.30423808098\n",
      "Epoch 42 Cost 2.30423784256\n",
      "Epoch 43 Cost 2.30423688889\n",
      "Epoch 44 Cost 2.30423855782\n",
      "Epoch 45 Cost 2.3042383194\n",
      "Epoch 46 Cost 2.30423760414\n",
      "Epoch 47 Cost 2.30423593521\n",
      "Epoch 48 Cost 2.30423665047\n",
      "Epoch 49 Cost 2.30423736572\n",
      "Epoch 50 Cost 2.3042371273\n",
      "Epoch 51 Cost 2.30423688889\n",
      "Epoch 52 Cost 2.30423760414\n",
      "Epoch 53 Cost 2.3042371273\n",
      "Epoch 54 Cost 2.3042383194\n",
      "Epoch 55 Cost 2.30423545837\n",
      "Epoch 56 Cost 2.30423927307\n",
      "Epoch 57 Cost 2.30423665047\n",
      "Epoch 58 Cost 2.30423855782\n",
      "Epoch 59 Cost 2.3042383194\n",
      "Epoch 60 Cost 2.30423879623\n",
      "Epoch 61 Cost 2.30423927307\n",
      "Epoch 62 Cost 2.30423736572\n",
      "Epoch 63 Cost 2.3042371273\n",
      "Epoch 64 Cost 2.30423688889\n",
      "Epoch 65 Cost 2.3042371273\n",
      "Epoch 66 Cost 2.30423688889\n",
      "Epoch 67 Cost 2.30423736572\n",
      "Epoch 68 Cost 2.30423736572\n",
      "Epoch 69 Cost 2.30423617363\n",
      "Epoch 70 Cost 2.30423879623\n",
      "Epoch 71 Cost 2.30423784256\n",
      "Epoch 72 Cost 2.3042371273\n",
      "Epoch 73 Cost 2.30423617363\n",
      "Epoch 74 Cost 2.3042383194\n",
      "Epoch 75 Cost 2.30423688889\n",
      "Epoch 76 Cost 2.30423593521\n",
      "Epoch 77 Cost 2.30423641205\n",
      "Epoch 78 Cost 2.30423617363\n",
      "Epoch 79 Cost 2.30423736572\n",
      "Epoch 80 Cost 2.30423545837\n",
      "Epoch 81 Cost 2.30423545837\n",
      "Epoch 82 Cost 2.30423688889\n",
      "Epoch 83 Cost 2.30423808098\n",
      "Epoch 84 Cost 2.30423688889\n",
      "Epoch 85 Cost 2.30423688889\n",
      "Epoch 86 Cost 2.3042383194\n",
      "Epoch 87 Cost 2.3042383194\n",
      "Epoch 88 Cost 2.30423879623\n",
      "Epoch 89 Cost 2.3042345047\n",
      "Epoch 90 Cost 2.3042371273\n",
      "Epoch 91 Cost 2.30423736572\n",
      "Epoch 92 Cost 2.30423688889\n",
      "Epoch 93 Cost 2.30423641205\n",
      "Epoch 94 Cost 2.30423593521\n",
      "Epoch 95 Cost 2.30423665047\n",
      "Epoch 96 Cost 2.30423665047\n",
      "Epoch 97 Cost 2.30423760414\n",
      "Epoch 98 Cost 2.30423784256\n",
      "Epoch 99 Cost 2.30423617363\n"
     ]
    }
   ],
   "source": [
    "for epoch in xrange(100):\n",
    "    cost_vals = []\n",
    "    for bi in xrange(n_batches):\n",
    "        cost_val = train_model(bi)\n",
    "        cost_vals.append(cost_val)\n",
    "    print(\"Epoch {} Cost {}\".format(epoch, np.mean(cost_vals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
